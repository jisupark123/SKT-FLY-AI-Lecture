{"cells":[{"cell_type":"markdown","id":"250b088e","metadata":{"id":"250b088e"},"source":["# DQN으로 Shooring Airplane Game 강화학습\n","\n","먼저 여러가지 설정 변수 정의"]},{"cell_type":"code","execution_count":32,"id":"be000272-b287-41c7-b5ed-1bd352af1a71","metadata":{"id":"be000272-b287-41c7-b5ed-1bd352af1a71","tags":[]},"outputs":[],"source":["import numpy as np\n","import gym\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as T\n","\n","# Configuration paramaters for the whole setup\n","seed = 42\n","gamma = 0.99  # Discount factor for past rewards\n","epsilon = 1.0  # Epsilon greedy parameter\n","epsilon_min = 0.1  # Minimum epsilon greedy parameter\n","epsilon_max = 1.0  # Maximum epsilon greedy parameter\n","epsilon_interval = epsilon_max - epsilon_min # Rate at which to reduce chance\n","                                             # of random action being taken\n","batch_size = 16  # Size of batch taken from replay buffer\n","max_steps_per_episode = 60\n","max_episodes = 5000"]},{"cell_type":"markdown","id":"82a3fe92","metadata":{"id":"82a3fe92"},"source":["### 게임 환경 설정\n","\n","상태(state) 정의\n","- 보드판의 모양: (8 * 8) 행렬 * 3 채널\n","- 채널 0: unseen\n","- 채널 1: hit\n","- 채녈 2: miss\n","\n","액션 정의\n","- 돌의 가능한 위치 (8 * 8 = 64)"]},{"cell_type":"code","execution_count":33,"id":"5bdb4550-1558-4ba7-a910-9faa34652511","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1704373342536,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"},"user_tz":-540},"id":"5bdb4550-1558-4ba7-a910-9faa34652511","outputId":"daa6593f-2a88-4711-ec4b-4e6fa2aa956a","tags":[]},"outputs":[],"source":["env = gym.make('gym_examples:gym_examples/ShootingAirplane-v0', render_mode=\"text\")"]},{"cell_type":"markdown","id":"bfab303e","metadata":{"id":"bfab303e"},"source":["env에서 정의한 action_space, observation_space 의 모양 확인\n","- action_space: 3개의 값의 튜플 (벡터)\n","- observation_space: HWC 형태의 이미지 (마지막 축이 단일 값인 15 * 15 * 1 텐서) -> pytorch를 사용할 경우 적절히 1 * 15 * 15 텐서로 수정필요"]},{"cell_type":"code","execution_count":34,"id":"d3ba8e44","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1704373342536,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"},"user_tz":-540},"id":"d3ba8e44","outputId":"46cd5ede-d57e-4cba-b00f-d0209fca31e5"},"outputs":[{"data":{"text/plain":["(2,)"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["env.action_space.shape"]},{"cell_type":"code","execution_count":35,"id":"d4c3c91f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1704373343757,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"},"user_tz":-540},"id":"d4c3c91f","outputId":"7db97b40-f3a6-4712-aac2-e75d37c69779"},"outputs":[{"data":{"text/plain":["(8, 8, 1)"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["env.observation_space.shape"]},{"cell_type":"markdown","id":"5477cab4","metadata":{"id":"5477cab4"},"source":["### 네트워크 정의하기\n","\n","참고: Conv2d 파라미터\n","* in_channels (int) – Number of channels in the input image\n","* out_channels (int) – Number of channels produced by the convolution\n","* kernel_size (int or tuple) – Size of the convolving kernel\n","* stride (int or tuple, optional) – Stride of the convolution. Default: 1\n","* padding (int, tuple or str, optional) – Padding added to all four sides of the input. Default: 0\n","* padding_mode (str, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'"]},{"cell_type":"code","execution_count":36,"id":"f85ef96c-cc48-426a-b2f3-12e002502bc9","metadata":{"id":"f85ef96c-cc48-426a-b2f3-12e002502bc9","tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","num_actions = 64\n","\n","class QModel(nn.Module):\n","    def __init__(self, num_actions):\n","        super(QModel, self).__init__()\n","        self.dropout = nn.Dropout(p=0.3)\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding='same')\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding='same')\n","        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(1152, 512)\n","        self.fc2 = nn.Linear(512, num_actions)\n","\n","    def forward(self, x):\n","        x = nn.functional.relu(self.conv1(x))\n","        x = nn.functional.relu(self.conv2(x))\n","        x = self.dropout(x)\n","        x = nn.functional.relu(self.conv3(x))\n","        x = self.flatten(x)\n","        x = nn.functional.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        action = self.fc2(x)\n","        return action"]},{"cell_type":"markdown","id":"21a23e4c","metadata":{"id":"21a23e4c"},"source":["### 모델 빌딩 & 로스 및 최적화 계산기 만들기"]},{"cell_type":"code","execution_count":37,"id":"8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f","metadata":{"id":"8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# The first model makes the predictions for Q-values which are used to\n","# make a action.\n","model = QModel(num_actions)\n","model.to(device)\n","\n","# Build a target model for the prediction of future rewards.\n","# The weights of a target model get updated every 10000 steps thus when the\n","# loss between the Q-values is calculated the target Q-value is stable.\n","model_target = QModel(num_actions)\n","model_target.to(device)\n","\n","loss_function = nn.SmoothL1Loss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)"]},{"cell_type":"code","execution_count":38,"id":"38d7c152","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":366,"status":"ok","timestamp":1704373352803,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"},"user_tz":-540},"id":"38d7c152","outputId":"4fc302d5-2b42-4f92-e393-3b7e9cc14263"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"markdown","id":"a859f550","metadata":{"id":"a859f550"},"source":["### Replay Buffer 정의"]},{"cell_type":"code","execution_count":39,"id":"8eed8ce4-5a53-457f-bc40-fda577b2ec29","metadata":{"id":"8eed8ce4-5a53-457f-bc40-fda577b2ec29"},"outputs":[],"source":["# Experience replay buffers\n","action_history = []\n","action_mask_history = []\n","state_history = []\n","state_next_history = []\n","rewards_history = []\n","done_history = []\n","episode_reward_history = []\n","running_reward = 0\n","episode_count = 0\n","frame_count = 0\n","\n","# Number of frames to take random action and observe output\n","epsilon_random_frames = 50000\n","# Number of frames for exploration\n","epsilon_greedy_frames = 200000.0\n","# Maximum replay length\n","# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n","max_memory_length = 500000\n","# Train the model after 4 actions\n","update_after_actions = 4\n","# How often to update the target network\n","update_target_network = 10000"]},{"cell_type":"markdown","id":"eda734a1","metadata":{"id":"eda734a1"},"source":["### 전처리\n","\n","- env가 리턴하는 observation은 일단 np.array이니 torch.tensor로 캐스팅\n","- env가 리턴하는 상태가 (8, 8, 1)의 HWC 이미지 텐서이므로 이를 (3, 15, 15)의 CHW 이미지로 변환\n","- One-hot 인코딩도 필요"]},{"cell_type":"code","execution_count":40,"id":"abf51fd4","metadata":{"id":"abf51fd4"},"outputs":[],"source":["# Function to preprocess the state\n","# note that player 1 = env player, player 2 = agent\n","def preprocess_state(env_observ):\n","    st = torch.from_numpy(env_observ).squeeze()\n","    st = st.to(torch.int64)\n","    st = torch.nn.functional.one_hot(st,num_classes=3)\n","    st = st.permute(2, 0, 1)\n","    return st.to(torch.float32)"]},{"cell_type":"markdown","id":"61693680","metadata":{"id":"61693680"},"source":["### Epsilon-greedy 액션 선택 함수\n","\n","학습시 에피소드 생성하면서 사용 (주의: 입력은 batch axis 없음)"]},{"cell_type":"code","execution_count":41,"id":"f3f9bd3c","metadata":{"id":"f3f9bd3c"},"outputs":[],"source":["# Function to select an action\n","# model: the torch model to compuate action-state value (i.e., q-value)\n","# state: a torch tensor (3 x 8 x 8) of float32, which is output by preprocess_state\n","# mask: a 64-size array (np.array)\n","def get_greedy_epsilon(model, state, mask):\n","    global epsilon\n","\n","    #if frame_count < epsilon_random_frames or np.random.rand(1)[0] < epsilon:\n","    if np.random.rand(1)[0] < epsilon:\n","        action = np.random.choice([ i for i in range(num_actions) if mask[i] == 1 ])\n","    else:\n","        with torch.no_grad():\n","            # add a batch axis\n","            state_tensor = state.unsqueeze(0)\n","            # compute the q-values\n","            q_values = model(state_tensor)\n","            # select the q-values of valid actions\n","            action = torch.argmax(\n","                q_values.to('cpu').squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n","                dim=0)\n","\n","\n","            #valid_q = [ (i, q_values[0][i]) for i in range(64) if mask[i] == 1 ]\n","            # the action of maximum q-value\n","            #action, _ = max(valid_q, key=lambda e: e[1])\n","\n","    # decay epsilon\n","    epsilon -= epsilon_interval / epsilon_greedy_frames\n","    epsilon = max(epsilon, epsilon_min)\n","\n","    return action"]},{"cell_type":"markdown","id":"ea6728b0","metadata":{"id":"ea6728b0"},"source":["### Greedy 액션 선택 함수\n","\n","나중에 evaluation 시 사용"]},{"cell_type":"code","execution_count":42,"id":"8025c8a5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1704374107544,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"},"user_tz":-540},"id":"8025c8a5","outputId":"41d30158-8ea9-4bd3-cf34-cb85980b5630"},"outputs":[],"source":["def get_greedy_action(model, state, mask):\n","    global epsilon\n","\n","    with torch.no_grad():\n","        state_tensor = state.unsqueeze(0) # batch dimension\n","        q_values = model(state_tensor)\n","\n","        action = torch.argmax(\n","                q_values.to('cpu').squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n","                dim=0)\n","\n","    return action"]},{"cell_type":"markdown","id":"b99ac400","metadata":{"id":"b99ac400"},"source":["### Update 파트\n","\n","- Replay buffer 에서 batch하나를 샘플링하고,\n","- model을 update한다."]},{"cell_type":"code","execution_count":43,"id":"e49c6175","metadata":{"id":"e49c6175"},"outputs":[],"source":["# sample a batch of _batch_size from replay buffers\n","# return numpy.ndarrays\n","def sample_batch(_batch_size):\n","    # Get indices of samples for replay buffers\n","    indices = np.random.choice(range(len(done_history)), size=_batch_size, replace=False)\n","\n","    state_sample = np.array([state_history[i].squeeze(0).numpy() for i in indices])\n","    state_next_sample = np.array([state_next_history[i].squeeze(0).numpy() for i in indices])\n","    rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n","    action_sample = np.array([action_history[i] for i in indices])\n","\n","    # action mask is the mask for the valid actions at the '''next''' state\n","    action_mask_sample = np.array([action_mask_history[i] for i in indices])\n","    done_sample = np.array([float(done_history[i]) for i in indices])\n","\n","    return state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample"]},{"cell_type":"code","execution_count":44,"id":"b97e7ba6","metadata":{"id":"b97e7ba6"},"outputs":[],"source":["# Function to update the Q-network\n","def update_network():\n","    # sample a batch of ...\n","    state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample = \\\n","        sample_batch(batch_size)\n","\n","    # Convert numpy arrays to PyTorch tensors\n","    state_sample = torch.tensor(state_sample, dtype=torch.float32).to(device)\n","    state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32).to(device)\n","    action_sample = torch.tensor(action_sample, dtype=torch.int64).to(device)\n","    action_mask_sample = torch.tensor(action_mask_sample, dtype=torch.int64).to(device)\n","    rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32).to(device)\n","    done_sample = torch.tensor(done_sample, dtype=torch.float32).to(device)\n","\n","    # Compute the target Q-values for the states\n","    with torch.no_grad():\n","        future_rewards = model_target(state_next_sample)\n","        #future_rewards = future_rewards.cpu()\n","\n","        # compute the q-value for the next state and the action maximizing the q-value\n","        # note: the action should be valid (i.e., mask is set to 1)\n","        max_q_values = torch.max(\n","            future_rewards + action_mask_sample * 100., # trick to select a valid action\n","            dim=1).values.detach() - 100.\n","\n","        # compute the target q-value\n","        # if the step was final, max_q_values should not be added\n","        # we assume that the negative return of the opposite player is the return of next step\n","        # that is, G(t) = r(t+1) - g*r(t+2) + g^2*r(t+3) - g^3*r(t+4) + ...\n","        target_q_values = rewards_sample + gamma * max_q_values * (1. - done_sample)\n","\n","    # It's forward propagation! Compute the Q-values for the taken actions\n","    q_values = model(state_sample)\n","    #q_values = q_values.cpu()\n","    q_values_action = q_values.gather(dim=1, index=action_sample.unsqueeze(1)).squeeze(1)\n","\n","    # Compute the loss\n","    loss = loss_function(q_values_action, target_q_values)\n","\n","    # Perform the optimization step\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"markdown","id":"90ed4dcc-f24e-4990-879a-c2f6af51a063","metadata":{"id":"90ed4dcc-f24e-4990-879a-c2f6af51a063"},"source":["# Run DQN Tranining"]},{"cell_type":"code","execution_count":45,"id":"43c0a265","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1358572,"status":"ok","timestamp":1704375529858,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"},"user_tz":-540},"id":"43c0a265","outputId":"15bbae9c-3f89-4a38-bac7-a4fd182557a8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/anaconda3/envs/test/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"name":"stdout","output_type":"stream","text":["Episode: 10, Frame count: 579, Running reward: -39.1\n","Episode: 20, Frame count: 1153, Running reward: -39.15\n","Episode: 30, Frame count: 1704, Running reward: -38.06666666666667\n","Episode: 40, Frame count: 2285, Running reward: -38.725\n","Episode: 50, Frame count: 2824, Running reward: -38.04\n","Episode: 60, Frame count: 3377, Running reward: -37.71666666666667\n","Episode: 70, Frame count: 3928, Running reward: -37.48571428571429\n","Episode: 80, Frame count: 4507, Running reward: -37.7375\n","Episode: 90, Frame count: 5080, Running reward: -37.93333333333333\n","Episode: 100, Frame count: 5652, Running reward: -38.04\n","Episode: 110, Frame count: 6216, Running reward: -37.83\n","Episode: 120, Frame count: 6773, Running reward: -37.66\n","Episode: 130, Frame count: 7346, Running reward: -37.96\n","Episode: 140, Frame count: 7901, Running reward: -37.54\n","Episode: 150, Frame count: 8463, Running reward: -37.69\n","Episode: 160, Frame count: 9036, Running reward: -37.93\n","Episode: 170, Frame count: 9587, Running reward: -37.99\n","Episode: 180, Frame count: 10150, Running reward: -37.77\n","Episode: 190, Frame count: 10711, Running reward: -37.51\n","Episode: 200, Frame count: 11261, Running reward: -37.19\n","Episode: 210, Frame count: 11827, Running reward: -37.23\n","Episode: 220, Frame count: 12358, Running reward: -36.89\n","Episode: 230, Frame count: 12926, Running reward: -36.84\n","Episode: 240, Frame count: 13477, Running reward: -36.76\n","Episode: 250, Frame count: 14019, Running reward: -36.58\n","Episode: 260, Frame count: 14558, Running reward: -36.14\n","Episode: 270, Frame count: 15087, Running reward: -35.84\n","Episode: 280, Frame count: 15629, Running reward: -35.53\n","Episode: 290, Frame count: 16186, Running reward: -35.53\n","Episode: 300, Frame count: 16692, Running reward: -35.05\n","Episode: 310, Frame count: 17204, Running reward: -34.45\n","Episode: 320, Frame count: 17715, Running reward: -34.17\n","Episode: 330, Frame count: 18256, Running reward: -33.9\n","Episode: 340, Frame count: 18778, Running reward: -33.57\n","Episode: 350, Frame count: 19262, Running reward: -32.93\n","Episode: 360, Frame count: 19813, Running reward: -33.09\n","Episode: 370, Frame count: 20337, Running reward: -33.02\n","Episode: 380, Frame count: 20880, Running reward: -33.05\n","Episode: 390, Frame count: 21405, Running reward: -32.65\n","Episode: 400, Frame count: 21932, Running reward: -32.86\n","Episode: 410, Frame count: 22398, Running reward: -32.4\n","Episode: 420, Frame count: 22877, Running reward: -32.14\n","Episode: 430, Frame count: 23353, Running reward: -31.35\n","Episode: 440, Frame count: 23864, Running reward: -31.24\n","Episode: 450, Frame count: 24337, Running reward: -31.17\n","Episode: 460, Frame count: 24849, Running reward: -30.78\n","Episode: 470, Frame count: 25372, Running reward: -30.75\n","Episode: 480, Frame count: 25884, Running reward: -30.46\n","Episode: 490, Frame count: 26373, Running reward: -30.14\n","Episode: 500, Frame count: 26880, Running reward: -29.9\n","Episode: 510, Frame count: 27345, Running reward: -29.89\n","Episode: 520, Frame count: 27844, Running reward: -30.07\n","Episode: 530, Frame count: 28368, Running reward: -30.57\n","Episode: 540, Frame count: 28907, Running reward: -30.85\n","Episode: 550, Frame count: 29353, Running reward: -30.52\n","Episode: 560, Frame count: 29881, Running reward: -30.68\n","Episode: 570, Frame count: 30381, Running reward: -30.45\n","Episode: 580, Frame count: 30837, Running reward: -29.85\n","Episode: 590, Frame count: 31283, Running reward: -29.36\n","Episode: 600, Frame count: 31743, Running reward: -28.93\n","Episode: 610, Frame count: 32192, Running reward: -28.77\n","Episode: 620, Frame count: 32682, Running reward: -28.64\n","Episode: 630, Frame count: 33150, Running reward: -28.08\n","Episode: 640, Frame count: 33631, Running reward: -27.5\n","Episode: 650, Frame count: 34096, Running reward: -27.69\n","Episode: 660, Frame count: 34541, Running reward: -26.82\n","Episode: 670, Frame count: 35025, Running reward: -26.62\n","Episode: 680, Frame count: 35483, Running reward: -26.66\n","Episode: 690, Frame count: 35953, Running reward: -26.9\n","Episode: 700, Frame count: 36374, Running reward: -26.47\n","Episode: 710, Frame count: 36808, Running reward: -26.3\n","Episode: 720, Frame count: 37237, Running reward: -25.67\n","Episode: 730, Frame count: 37744, Running reward: -26.06\n","Episode: 740, Frame count: 38189, Running reward: -25.68\n","Episode: 750, Frame count: 38592, Running reward: -25.06\n","Episode: 760, Frame count: 39035, Running reward: -25.02\n","Episode: 770, Frame count: 39499, Running reward: -24.82\n","Episode: 780, Frame count: 39924, Running reward: -24.47\n","Episode: 790, Frame count: 40306, Running reward: -23.57\n","Episode: 800, Frame count: 40775, Running reward: -24.05\n","Episode: 810, Frame count: 41242, Running reward: -24.38\n","Episode: 820, Frame count: 41695, Running reward: -24.62\n","Episode: 830, Frame count: 42128, Running reward: -23.84\n","Episode: 840, Frame count: 42569, Running reward: -23.8\n","Episode: 850, Frame count: 43036, Running reward: -24.46\n","Episode: 860, Frame count: 43525, Running reward: -24.92\n","Episode: 870, Frame count: 44012, Running reward: -25.19\n","Episode: 880, Frame count: 44406, Running reward: -24.88\n","Episode: 890, Frame count: 44835, Running reward: -25.35\n","Episode: 900, Frame count: 45257, Running reward: -24.88\n","Episode: 910, Frame count: 45725, Running reward: -24.93\n","Episode: 920, Frame count: 46196, Running reward: -25.11\n","Episode: 930, Frame count: 46669, Running reward: -25.55\n","Episode: 940, Frame count: 47147, Running reward: -25.96\n","Episode: 950, Frame count: 47578, Running reward: -25.6\n","Episode: 960, Frame count: 48008, Running reward: -25.01\n","Episode: 970, Frame count: 48479, Running reward: -24.83\n","Episode: 980, Frame count: 48944, Running reward: -25.56\n","Episode: 990, Frame count: 49388, Running reward: -25.71\n","Episode: 1000, Frame count: 49773, Running reward: -25.34\n","Episode: 1010, Frame count: 50256, Running reward: -25.45\n","Episode: 1020, Frame count: 50651, Running reward: -24.69\n","Episode: 1030, Frame count: 51066, Running reward: -24.07\n","Episode: 1040, Frame count: 51505, Running reward: -23.64\n","Episode: 1050, Frame count: 51924, Running reward: -23.52\n","Episode: 1060, Frame count: 52355, Running reward: -23.53\n","Episode: 1070, Frame count: 52746, Running reward: -22.71\n","Episode: 1080, Frame count: 53153, Running reward: -22.11\n","Episode: 1090, Frame count: 53531, Running reward: -21.45\n","Episode: 1100, Frame count: 53925, Running reward: -21.56\n","Episode: 1110, Frame count: 54338, Running reward: -20.86\n","Episode: 1120, Frame count: 54736, Running reward: -20.89\n","Episode: 1130, Frame count: 55150, Running reward: -20.88\n","Episode: 1140, Frame count: 55536, Running reward: -20.35\n","Episode: 1150, Frame count: 55934, Running reward: -20.12\n","Episode: 1160, Frame count: 56313, Running reward: -19.6\n","Episode: 1170, Frame count: 56707, Running reward: -19.65\n","Episode: 1180, Frame count: 57118, Running reward: -19.69\n","Episode: 1190, Frame count: 57505, Running reward: -19.78\n","Episode: 1200, Frame count: 57913, Running reward: -19.9\n","Episode: 1210, Frame count: 58275, Running reward: -19.39\n","Episode: 1220, Frame count: 58631, Running reward: -18.97\n","Episode: 1230, Frame count: 59029, Running reward: -18.83\n","Episode: 1240, Frame count: 59418, Running reward: -18.86\n","Episode: 1250, Frame count: 59789, Running reward: -18.59\n","Episode: 1260, Frame count: 60169, Running reward: -18.6\n","Episode: 1270, Frame count: 60553, Running reward: -18.5\n","Episode: 1280, Frame count: 60950, Running reward: -18.36\n","Episode: 1290, Frame count: 61345, Running reward: -18.44\n","Episode: 1300, Frame count: 61697, Running reward: -17.88\n","Episode: 1310, Frame count: 62081, Running reward: -18.1\n","Episode: 1320, Frame count: 62515, Running reward: -18.9\n","Episode: 1330, Frame count: 62896, Running reward: -18.71\n","Episode: 1340, Frame count: 63333, Running reward: -19.21\n","Episode: 1350, Frame count: 63750, Running reward: -19.69\n","Episode: 1360, Frame count: 64143, Running reward: -19.82\n","Episode: 1370, Frame count: 64541, Running reward: -19.94\n","Episode: 1380, Frame count: 64910, Running reward: -19.66\n","Episode: 1390, Frame count: 65311, Running reward: -19.74\n","Episode: 1400, Frame count: 65718, Running reward: -20.31\n","Episode: 1410, Frame count: 66113, Running reward: -20.44\n","Episode: 1420, Frame count: 66501, Running reward: -19.96\n","Episode: 1430, Frame count: 66926, Running reward: -20.4\n","Episode: 1440, Frame count: 67295, Running reward: -19.7\n","Episode: 1450, Frame count: 67662, Running reward: -19.18\n","Episode: 1460, Frame count: 68036, Running reward: -18.99\n","Episode: 1470, Frame count: 68382, Running reward: -18.47\n","Episode: 1480, Frame count: 68739, Running reward: -18.35\n","Episode: 1490, Frame count: 69154, Running reward: -18.47\n","Episode: 1500, Frame count: 69570, Running reward: -18.56\n","Episode: 1510, Frame count: 69948, Running reward: -18.37\n","Episode: 1520, Frame count: 70313, Running reward: -18.14\n","Episode: 1530, Frame count: 70678, Running reward: -17.54\n","Episode: 1540, Frame count: 71003, Running reward: -17.1\n","Episode: 1550, Frame count: 71440, Running reward: -17.8\n","Episode: 1560, Frame count: 71785, Running reward: -17.51\n","Episode: 1570, Frame count: 72129, Running reward: -17.49\n","Episode: 1580, Frame count: 72502, Running reward: -17.65\n","Episode: 1590, Frame count: 72866, Running reward: -17.14\n","Episode: 1600, Frame count: 73265, Running reward: -16.95\n","Episode: 1610, Frame count: 73593, Running reward: -16.45\n","Episode: 1620, Frame count: 73998, Running reward: -16.87\n","Episode: 1630, Frame count: 74363, Running reward: -16.87\n","Episode: 1640, Frame count: 74713, Running reward: -17.12\n","Episode: 1650, Frame count: 75054, Running reward: -16.16\n","Episode: 1660, Frame count: 75470, Running reward: -16.87\n","Episode: 1670, Frame count: 75881, Running reward: -17.54\n","Episode: 1680, Frame count: 76240, Running reward: -17.4\n","Episode: 1690, Frame count: 76639, Running reward: -17.75\n","Episode: 1700, Frame count: 76982, Running reward: -17.19\n","Episode: 1710, Frame count: 77321, Running reward: -17.3\n","Episode: 1720, Frame count: 77702, Running reward: -17.04\n","Episode: 1730, Frame count: 78038, Running reward: -16.75\n","Episode: 1740, Frame count: 78396, Running reward: -16.83\n","Episode: 1750, Frame count: 78776, Running reward: -17.22\n","Episode: 1760, Frame count: 79112, Running reward: -16.42\n","Episode: 1770, Frame count: 79456, Running reward: -15.75\n","Episode: 1780, Frame count: 79807, Running reward: -15.67\n","Episode: 1790, Frame count: 80117, Running reward: -14.78\n","Episode: 1800, Frame count: 80481, Running reward: -14.99\n","Episode: 1810, Frame count: 80833, Running reward: -15.12\n","Episode: 1820, Frame count: 81232, Running reward: -15.3\n","Episode: 1830, Frame count: 81552, Running reward: -15.14\n","Episode: 1840, Frame count: 81890, Running reward: -14.94\n","Episode: 1850, Frame count: 82244, Running reward: -14.68\n","Episode: 1860, Frame count: 82612, Running reward: -15.0\n","Episode: 1870, Frame count: 82926, Running reward: -14.7\n","Episode: 1880, Frame count: 83290, Running reward: -14.83\n","Episode: 1890, Frame count: 83600, Running reward: -14.83\n","Episode: 1900, Frame count: 83908, Running reward: -14.27\n","Episode: 1910, Frame count: 84209, Running reward: -13.76\n","Episode: 1920, Frame count: 84530, Running reward: -12.98\n","Episode: 1930, Frame count: 84891, Running reward: -13.39\n","Episode: 1940, Frame count: 85234, Running reward: -13.44\n","Episode: 1950, Frame count: 85522, Running reward: -12.78\n","Episode: 1960, Frame count: 85817, Running reward: -12.05\n","Episode: 1970, Frame count: 86124, Running reward: -11.98\n","Episode: 1980, Frame count: 86415, Running reward: -11.25\n","Episode: 1990, Frame count: 86799, Running reward: -12.01\n","Episode: 2000, Frame count: 87138, Running reward: -12.32\n","Episode: 2010, Frame count: 87474, Running reward: -12.67\n","Episode: 2020, Frame count: 87834, Running reward: -13.06\n","Episode: 2030, Frame count: 88132, Running reward: -12.43\n","Episode: 2040, Frame count: 88446, Running reward: -12.14\n","Episode: 2050, Frame count: 88773, Running reward: -12.53\n","Episode: 2060, Frame count: 89085, Running reward: -12.7\n","Episode: 2070, Frame count: 89434, Running reward: -13.12\n","Episode: 2080, Frame count: 89774, Running reward: -13.61\n","Episode: 2090, Frame count: 90101, Running reward: -13.02\n","Episode: 2100, Frame count: 90428, Running reward: -12.9\n","Episode: 2110, Frame count: 90753, Running reward: -12.79\n","Episode: 2120, Frame count: 91050, Running reward: -12.16\n","Episode: 2130, Frame count: 91385, Running reward: -12.53\n","Episode: 2140, Frame count: 91671, Running reward: -12.25\n","Episode: 2150, Frame count: 91949, Running reward: -11.76\n","Episode: 2160, Frame count: 92297, Running reward: -12.12\n","Episode: 2170, Frame count: 92640, Running reward: -12.06\n","Episode: 2180, Frame count: 92996, Running reward: -12.22\n","Episode: 2190, Frame count: 93304, Running reward: -12.03\n","Episode: 2200, Frame count: 93580, Running reward: -11.52\n","Episode: 2210, Frame count: 93901, Running reward: -11.48\n","Episode: 2220, Frame count: 94201, Running reward: -11.51\n","Episode: 2230, Frame count: 94487, Running reward: -11.02\n","Episode: 2240, Frame count: 94777, Running reward: -11.06\n","Episode: 2250, Frame count: 95078, Running reward: -11.29\n","Episode: 2260, Frame count: 95386, Running reward: -10.89\n","Episode: 2270, Frame count: 95689, Running reward: -10.49\n","Episode: 2280, Frame count: 96003, Running reward: -10.07\n","Episode: 2290, Frame count: 96291, Running reward: -9.87\n","Episode: 2300, Frame count: 96611, Running reward: -10.31\n","Episode: 2310, Frame count: 96928, Running reward: -10.27\n","Episode: 2320, Frame count: 97243, Running reward: -10.42\n","Episode: 2330, Frame count: 97558, Running reward: -10.71\n","Episode: 2340, Frame count: 97848, Running reward: -10.71\n","Episode: 2350, Frame count: 98178, Running reward: -11.0\n","Episode: 2360, Frame count: 98481, Running reward: -10.95\n","Episode: 2370, Frame count: 98784, Running reward: -10.95\n","Episode: 2380, Frame count: 99075, Running reward: -10.72\n","Episode: 2390, Frame count: 99332, Running reward: -10.41\n","Episode: 2400, Frame count: 99620, Running reward: -10.09\n","Episode: 2410, Frame count: 99884, Running reward: -9.56\n","Episode: 2420, Frame count: 100190, Running reward: -9.47\n","Episode: 2430, Frame count: 100493, Running reward: -9.35\n","Episode: 2440, Frame count: 100793, Running reward: -9.45\n","Episode: 2450, Frame count: 101071, Running reward: -8.93\n","Episode: 2460, Frame count: 101375, Running reward: -8.94\n","Episode: 2470, Frame count: 101639, Running reward: -8.55\n","Episode: 2480, Frame count: 101897, Running reward: -8.22\n","Episode: 2490, Frame count: 102168, Running reward: -8.36\n","Episode: 2500, Frame count: 102465, Running reward: -8.45\n","Episode: 2510, Frame count: 102775, Running reward: -8.91\n","Episode: 2520, Frame count: 103042, Running reward: -8.52\n","Episode: 2530, Frame count: 103327, Running reward: -8.34\n","Episode: 2540, Frame count: 103628, Running reward: -8.35\n","Episode: 2550, Frame count: 103897, Running reward: -8.26\n","Episode: 2560, Frame count: 104179, Running reward: -8.04\n","Episode: 2570, Frame count: 104465, Running reward: -8.26\n","Episode: 2580, Frame count: 104739, Running reward: -8.42\n","Episode: 2590, Frame count: 105031, Running reward: -8.63\n","Episode: 2600, Frame count: 105334, Running reward: -8.69\n","Episode: 2610, Frame count: 105636, Running reward: -8.61\n","Episode: 2620, Frame count: 105910, Running reward: -8.68\n","Episode: 2630, Frame count: 106209, Running reward: -8.82\n","Episode: 2640, Frame count: 106484, Running reward: -8.56\n","Episode: 2650, Frame count: 106775, Running reward: -8.78\n","Episode: 2660, Frame count: 107043, Running reward: -8.64\n","Episode: 2670, Frame count: 107302, Running reward: -8.37\n","Episode: 2680, Frame count: 107589, Running reward: -8.5\n","Episode: 2690, Frame count: 107835, Running reward: -8.04\n","Episode: 2700, Frame count: 108105, Running reward: -7.71\n","Episode: 2710, Frame count: 108376, Running reward: -7.4\n","Episode: 2720, Frame count: 108654, Running reward: -7.44\n","Episode: 2730, Frame count: 108930, Running reward: -7.21\n","Episode: 2740, Frame count: 109230, Running reward: -7.46\n","Episode: 2750, Frame count: 109475, Running reward: -7.0\n","Episode: 2760, Frame count: 109753, Running reward: -7.1\n","Episode: 2770, Frame count: 110027, Running reward: -7.25\n","Episode: 2780, Frame count: 110293, Running reward: -7.04\n","Episode: 2790, Frame count: 110578, Running reward: -7.43\n","Episode: 2800, Frame count: 110861, Running reward: -7.56\n","Episode: 2810, Frame count: 111119, Running reward: -7.43\n","Episode: 2820, Frame count: 111364, Running reward: -7.1\n","Episode: 2830, Frame count: 111666, Running reward: -7.36\n","Episode: 2840, Frame count: 111928, Running reward: -6.98\n","Episode: 2850, Frame count: 112210, Running reward: -7.35\n","Episode: 2860, Frame count: 112507, Running reward: -7.54\n","Episode: 2870, Frame count: 112797, Running reward: -7.7\n","Episode: 2880, Frame count: 113070, Running reward: -7.77\n","Episode: 2890, Frame count: 113336, Running reward: -7.58\n","Episode: 2900, Frame count: 113598, Running reward: -7.37\n","Episode: 2910, Frame count: 113863, Running reward: -7.44\n","Episode: 2920, Frame count: 114105, Running reward: -7.41\n","Episode: 2930, Frame count: 114363, Running reward: -6.97\n","Episode: 2940, Frame count: 114604, Running reward: -6.76\n","Episode: 2950, Frame count: 114855, Running reward: -6.45\n","Episode: 2960, Frame count: 115109, Running reward: -6.02\n","Episode: 2970, Frame count: 115378, Running reward: -5.81\n","Episode: 2980, Frame count: 115659, Running reward: -5.89\n","Episode: 2990, Frame count: 115912, Running reward: -5.76\n","Episode: 3000, Frame count: 116213, Running reward: -6.15\n","Episode: 3010, Frame count: 116466, Running reward: -6.03\n","Episode: 3020, Frame count: 116734, Running reward: -6.29\n","Episode: 3030, Frame count: 117004, Running reward: -6.41\n","Episode: 3040, Frame count: 117271, Running reward: -6.67\n","Episode: 3050, Frame count: 117564, Running reward: -7.09\n","Episode: 3060, Frame count: 117839, Running reward: -7.3\n","Episode: 3070, Frame count: 118147, Running reward: -7.69\n","Episode: 3080, Frame count: 118400, Running reward: -7.41\n","Episode: 3090, Frame count: 118637, Running reward: -7.25\n","Episode: 3100, Frame count: 118896, Running reward: -6.83\n","Episode: 3110, Frame count: 119196, Running reward: -7.3\n","Episode: 3120, Frame count: 119463, Running reward: -7.29\n","Episode: 3130, Frame count: 119749, Running reward: -7.45\n","Episode: 3140, Frame count: 119994, Running reward: -7.23\n","Episode: 3150, Frame count: 120259, Running reward: -6.95\n","Episode: 3160, Frame count: 120538, Running reward: -6.99\n","Episode: 3170, Frame count: 120767, Running reward: -6.2\n","Episode: 3180, Frame count: 121017, Running reward: -6.17\n","Episode: 3190, Frame count: 121276, Running reward: -6.39\n","Episode: 3200, Frame count: 121552, Running reward: -6.56\n","Episode: 3210, Frame count: 121776, Running reward: -5.8\n","Episode: 3220, Frame count: 122029, Running reward: -5.66\n","Episode: 3230, Frame count: 122313, Running reward: -5.64\n","Episode: 3240, Frame count: 122552, Running reward: -5.58\n","Episode: 3250, Frame count: 122800, Running reward: -5.41\n","Episode: 3260, Frame count: 123044, Running reward: -5.06\n","Episode: 3270, Frame count: 123293, Running reward: -5.26\n","Episode: 3280, Frame count: 123508, Running reward: -4.91\n","Episode: 3290, Frame count: 123771, Running reward: -4.95\n","Episode: 3300, Frame count: 124008, Running reward: -4.56\n","Episode: 3310, Frame count: 124251, Running reward: -4.75\n","Episode: 3320, Frame count: 124514, Running reward: -4.85\n","Episode: 3330, Frame count: 124791, Running reward: -4.78\n","Episode: 3340, Frame count: 125073, Running reward: -5.21\n","Episode: 3350, Frame count: 125286, Running reward: -4.86\n","Episode: 3360, Frame count: 125532, Running reward: -4.88\n","Episode: 3370, Frame count: 125808, Running reward: -5.15\n","Episode: 3380, Frame count: 126039, Running reward: -5.31\n","Episode: 3390, Frame count: 126279, Running reward: -5.08\n","Episode: 3400, Frame count: 126531, Running reward: -5.23\n","Episode: 3410, Frame count: 126796, Running reward: -5.45\n","Episode: 3420, Frame count: 127026, Running reward: -5.12\n","Episode: 3430, Frame count: 127255, Running reward: -4.64\n","Episode: 3440, Frame count: 127494, Running reward: -4.21\n","Episode: 3450, Frame count: 127774, Running reward: -4.88\n","Episode: 3460, Frame count: 128030, Running reward: -4.98\n","Episode: 3470, Frame count: 128273, Running reward: -4.65\n","Episode: 3480, Frame count: 128511, Running reward: -4.72\n","Episode: 3490, Frame count: 128749, Running reward: -4.7\n","Episode: 3500, Frame count: 128996, Running reward: -4.65\n","Episode: 3510, Frame count: 129235, Running reward: -4.39\n","Episode: 3520, Frame count: 129462, Running reward: -4.36\n","Episode: 3530, Frame count: 129707, Running reward: -4.52\n","Episode: 3540, Frame count: 129931, Running reward: -4.37\n","Episode: 3550, Frame count: 130164, Running reward: -3.9\n","Episode: 3560, Frame count: 130425, Running reward: -3.95\n","Episode: 3570, Frame count: 130649, Running reward: -3.76\n","Episode: 3580, Frame count: 130885, Running reward: -3.74\n","Episode: 3590, Frame count: 131132, Running reward: -3.83\n","Episode: 3600, Frame count: 131385, Running reward: -3.89\n","Episode: 3610, Frame count: 131618, Running reward: -3.83\n","Episode: 3620, Frame count: 131847, Running reward: -3.85\n","Episode: 3630, Frame count: 132081, Running reward: -3.74\n","Episode: 3640, Frame count: 132343, Running reward: -4.12\n","Episode: 3650, Frame count: 132594, Running reward: -4.3\n","Episode: 3660, Frame count: 132851, Running reward: -4.26\n","Episode: 3670, Frame count: 133078, Running reward: -4.29\n","Episode: 3680, Frame count: 133315, Running reward: -4.3\n","Episode: 3690, Frame count: 133554, Running reward: -4.22\n","Episode: 3700, Frame count: 133761, Running reward: -3.76\n","Episode: 3710, Frame count: 133979, Running reward: -3.61\n","Episode: 3720, Frame count: 134200, Running reward: -3.53\n","Episode: 3730, Frame count: 134426, Running reward: -3.45\n","Episode: 3740, Frame count: 134641, Running reward: -2.98\n","Episode: 3750, Frame count: 134873, Running reward: -2.79\n","Episode: 3760, Frame count: 135125, Running reward: -2.74\n","Episode: 3770, Frame count: 135361, Running reward: -2.83\n","Episode: 3780, Frame count: 135593, Running reward: -2.78\n","Episode: 3790, Frame count: 135842, Running reward: -2.88\n","Episode: 3800, Frame count: 136081, Running reward: -3.2\n","Episode: 3810, Frame count: 136319, Running reward: -3.4\n","Episode: 3820, Frame count: 136559, Running reward: -3.59\n","Episode: 3830, Frame count: 136787, Running reward: -3.61\n","Episode: 3840, Frame count: 137057, Running reward: -4.16\n","Episode: 3850, Frame count: 137287, Running reward: -4.14\n","Episode: 3860, Frame count: 137513, Running reward: -3.88\n","Episode: 3870, Frame count: 137734, Running reward: -3.73\n","Episode: 3880, Frame count: 137967, Running reward: -3.74\n","Episode: 3890, Frame count: 138196, Running reward: -3.54\n","Episode: 3900, Frame count: 138433, Running reward: -3.52\n","Episode: 3910, Frame count: 138661, Running reward: -3.42\n","Episode: 3920, Frame count: 138896, Running reward: -3.37\n","Episode: 3930, Frame count: 139147, Running reward: -3.6\n","Episode: 3940, Frame count: 139364, Running reward: -3.07\n","Episode: 3950, Frame count: 139576, Running reward: -2.89\n","Episode: 3960, Frame count: 139793, Running reward: -2.8\n","Episode: 3970, Frame count: 140001, Running reward: -2.67\n","Episode: 3980, Frame count: 140228, Running reward: -2.61\n","Episode: 3990, Frame count: 140452, Running reward: -2.56\n","Episode: 4000, Frame count: 140695, Running reward: -2.62\n","Episode: 4010, Frame count: 140910, Running reward: -2.49\n","Episode: 4020, Frame count: 141146, Running reward: -2.5\n","Episode: 4030, Frame count: 141400, Running reward: -2.53\n","Episode: 4040, Frame count: 141676, Running reward: -3.12\n","Episode: 4050, Frame count: 141925, Running reward: -3.49\n","Episode: 4060, Frame count: 142171, Running reward: -3.78\n","Episode: 4070, Frame count: 142413, Running reward: -4.12\n","Episode: 4080, Frame count: 142639, Running reward: -4.11\n","Episode: 4090, Frame count: 142860, Running reward: -4.08\n","Episode: 4100, Frame count: 143070, Running reward: -3.75\n","Episode: 4110, Frame count: 143313, Running reward: -4.03\n","Episode: 4120, Frame count: 143554, Running reward: -4.08\n","Episode: 4130, Frame count: 143782, Running reward: -3.82\n","Episode: 4140, Frame count: 144010, Running reward: -3.34\n","Episode: 4150, Frame count: 144241, Running reward: -3.16\n","Episode: 4160, Frame count: 144450, Running reward: -2.79\n","Episode: 4170, Frame count: 144689, Running reward: -2.76\n","Episode: 4180, Frame count: 144913, Running reward: -2.74\n","Episode: 4190, Frame count: 145124, Running reward: -2.64\n","Episode: 4200, Frame count: 145317, Running reward: -2.47\n","Episode: 4210, Frame count: 145533, Running reward: -2.2\n","Episode: 4220, Frame count: 145721, Running reward: -1.67\n","Episode: 4230, Frame count: 145927, Running reward: -1.45\n","Episode: 4240, Frame count: 146135, Running reward: -1.25\n","Episode: 4250, Frame count: 146342, Running reward: -1.01\n","Episode: 4260, Frame count: 146556, Running reward: -1.06\n","Episode: 4270, Frame count: 146782, Running reward: -0.93\n","Episode: 4280, Frame count: 147008, Running reward: -0.95\n","Episode: 4290, Frame count: 147220, Running reward: -0.96\n","Episode: 4300, Frame count: 147432, Running reward: -1.15\n","Episode: 4310, Frame count: 147643, Running reward: -1.1\n","Episode: 4320, Frame count: 147861, Running reward: -1.4\n","Episode: 4330, Frame count: 148079, Running reward: -1.52\n","Episode: 4340, Frame count: 148291, Running reward: -1.56\n","Episode: 4350, Frame count: 148489, Running reward: -1.47\n","Episode: 4360, Frame count: 148724, Running reward: -1.68\n","Episode: 4370, Frame count: 148942, Running reward: -1.6\n","Episode: 4380, Frame count: 149156, Running reward: -1.48\n","Episode: 4390, Frame count: 149372, Running reward: -1.52\n","Episode: 4400, Frame count: 149617, Running reward: -1.85\n","Episode: 4410, Frame count: 149822, Running reward: -1.79\n","Episode: 4420, Frame count: 150039, Running reward: -1.78\n","Episode: 4430, Frame count: 150255, Running reward: -1.76\n","Episode: 4440, Frame count: 150463, Running reward: -1.72\n","Episode: 4450, Frame count: 150676, Running reward: -1.87\n","Episode: 4460, Frame count: 150885, Running reward: -1.61\n","Episode: 4470, Frame count: 151116, Running reward: -1.74\n","Episode: 4480, Frame count: 151322, Running reward: -1.66\n","Episode: 4490, Frame count: 151533, Running reward: -1.61\n","Episode: 4500, Frame count: 151723, Running reward: -1.06\n","Episode: 4510, Frame count: 151926, Running reward: -1.04\n","Episode: 4520, Frame count: 152125, Running reward: -0.86\n","Episode: 4530, Frame count: 152352, Running reward: -0.97\n","Episode: 4540, Frame count: 152551, Running reward: -0.88\n","Episode: 4550, Frame count: 152742, Running reward: -0.66\n","Episode: 4560, Frame count: 152946, Running reward: -0.61\n","Episode: 4570, Frame count: 153156, Running reward: -0.4\n","Episode: 4580, Frame count: 153376, Running reward: -0.54\n","Episode: 4590, Frame count: 153568, Running reward: -0.35\n","Episode: 4600, Frame count: 153796, Running reward: -0.73\n","Episode: 4610, Frame count: 154010, Running reward: -0.84\n","Episode: 4620, Frame count: 154220, Running reward: -0.95\n","Episode: 4630, Frame count: 154422, Running reward: -0.7\n","Episode: 4640, Frame count: 154645, Running reward: -0.94\n","Episode: 4650, Frame count: 154838, Running reward: -0.96\n","Episode: 4660, Frame count: 155069, Running reward: -1.23\n","Episode: 4670, Frame count: 155270, Running reward: -1.14\n","Episode: 4680, Frame count: 155479, Running reward: -1.03\n","Episode: 4690, Frame count: 155692, Running reward: -1.24\n","Episode: 4700, Frame count: 155883, Running reward: -0.87\n","Episode: 4710, Frame count: 156103, Running reward: -0.93\n","Episode: 4720, Frame count: 156292, Running reward: -0.72\n","Episode: 4730, Frame count: 156508, Running reward: -0.86\n","Episode: 4740, Frame count: 156723, Running reward: -0.78\n","Episode: 4750, Frame count: 156940, Running reward: -1.02\n","Episode: 4760, Frame count: 157156, Running reward: -0.87\n","Episode: 4770, Frame count: 157360, Running reward: -0.9\n","Episode: 4780, Frame count: 157562, Running reward: -0.83\n","Episode: 4790, Frame count: 157788, Running reward: -0.96\n","Episode: 4800, Frame count: 157978, Running reward: -0.95\n","Episode: 4810, Frame count: 158231, Running reward: -1.28\n","Episode: 4820, Frame count: 158477, Running reward: -1.85\n","Episode: 4830, Frame count: 158687, Running reward: -1.79\n","Episode: 4840, Frame count: 158904, Running reward: -1.81\n","Episode: 4850, Frame count: 159118, Running reward: -1.78\n","Episode: 4860, Frame count: 159329, Running reward: -1.73\n","Episode: 4870, Frame count: 159527, Running reward: -1.67\n","Episode: 4880, Frame count: 159762, Running reward: -2.0\n","Episode: 4890, Frame count: 159983, Running reward: -1.95\n","Episode: 4900, Frame count: 160191, Running reward: -2.13\n","Episode: 4910, Frame count: 160432, Running reward: -2.01\n","Episode: 4920, Frame count: 160616, Running reward: -1.39\n","Episode: 4930, Frame count: 160815, Running reward: -1.28\n","Episode: 4940, Frame count: 161023, Running reward: -1.19\n","Episode: 4950, Frame count: 161235, Running reward: -1.17\n","Episode: 4960, Frame count: 161440, Running reward: -1.11\n","Episode: 4970, Frame count: 161649, Running reward: -1.22\n","Episode: 4980, Frame count: 161860, Running reward: -0.98\n","Episode: 4990, Frame count: 162047, Running reward: -0.64\n","Episode: 5000, Frame count: 162254, Running reward: -0.63\n"]}],"source":["for _ in range(max_episodes):\n","    state, info = env.reset()\n","    state = preprocess_state(state)\n","    action_mask = info['action_mask'].reshape((-1,))\n","    episode_reward = 0\n","\n","    for timestep in range(1, max_steps_per_episode):\n","        frame_count += 1\n","\n","        # Select an action\n","        #state_cuda = state.to(device)\n","        action = get_greedy_epsilon(model,\n","                      state.to(device),\n","                      action_mask)\n","        if action < 0:\n","            print(action_mask)\n","\n","        # Take the selected action\n","        state_next, reward, done, _, info = env.step((action // 8, action % 8))\n","        state_next = preprocess_state(state_next)\n","        action_mask = info['action_mask'].reshape((-1,))\n"," \n","        episode_reward += reward\n","\n","        # Store the transition in the replay buffer\n","        action_history.append(action)\n","        action_mask_history.append(action_mask)\n","        state_history.append(state)\n","        state_next_history.append(state_next)\n","        rewards_history.append(reward)\n","        done_history.append(done)\n","\n","        state = state_next\n","\n","        # Update every fourth frame and once batch size is over 32\n","        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n","            update_network()\n","\n","        if frame_count % update_target_network == 0:\n","            model_target.load_state_dict(model.state_dict())\n","\n","        # Limit the state and reward history\n","        if len(rewards_history) > max_memory_length:\n","            del rewards_history[:1]\n","            del state_history[:1]\n","            del state_next_history[:1]\n","            del action_history[:1]\n","            del action_mask_history[:1]\n","            del done_history[:1]\n","\n","        if done:\n","            break\n","\n","    episode_count += 1\n","    episode_reward_history.append(episode_reward)\n","\n","    # Update running reward to check condition for solving\n","    if len(episode_reward_history) > 100:\n","        del episode_reward_history[:1]\n","    running_reward = np.mean(episode_reward_history)\n","\n","    if episode_count % 10 == 0:\n","        print(f\"Episode: {episode_count}, Frame count: {frame_count}, Running reward: {running_reward}\")\n","\n","    if episode_count % 5000 == 0:\n","        torch.save(model, 'model.{}'.format(episode_count))\n","    #if running_reward > 20:\n","    #    print(f\"Solved at episode {episode_count}!\")\n","    #    break\n","\n","\n","torch.save(model, 'model.final')"]},{"cell_type":"markdown","id":"4984b880-e427-48cb-bf91-13a91d6529f5","metadata":{"id":"4984b880-e427-48cb-bf91-13a91d6529f5"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":46,"id":"5c198b4e-b0e4-4fd4-821d-be602c2dbc5a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16481,"status":"ok","timestamp":1704375618507,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"},"user_tz":-540},"id":"5c198b4e-b0e4-4fd4-821d-be602c2dbc5a","outputId":"7596f4ef-7354-43e4-aaa9-8624aa3decab"},"outputs":[{"name":"stdout","output_type":"stream","text":["    H    |     H   \n","  HHHHH  |   HHHHH \n"," MM HM   |     H   \n"," M HHHM  |    HHH  \n","    M    |         \n","    M    |         \n","         |         \n","         |         \n","\n"]}],"source":["import time, sys\n","from IPython.display import clear_output\n","\n","board, info = env.reset()\n","state = preprocess_state(board)\n","action_mask = info['action_mask'].reshape((-1,))\n","done = False\n","env.render()\n","\n","while not done:\n","    action = get_greedy_action(model, state.to(device), action_mask)\n","    print(\"action: ({}, {})\".format(action // 8, action % 8))\n","    sys.stdout.flush()\n","\n","    time.sleep(1.0)\n","    clear_output(wait=False)\n","    board, reward, done, _, info = env.step((action // 8, action % 8))\n","    state = preprocess_state(board)\n","    action_mask = info['action_mask'].reshape((-1,))\n","    env.render()"]},{"cell_type":"code","execution_count":null,"id":"p1QVpslLs8lS","metadata":{"id":"p1QVpslLs8lS"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":5}
