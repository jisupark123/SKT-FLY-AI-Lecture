{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 환경변수 세팅\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '8c91c5c7058847f3820f9d7c12bbb1fb'\n",
    "os.environ['AZURE_OPENAI_ENDPOINT'] = 'https://sktfly-ai.openai.azure.com/'\n",
    "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "os.environ['OPENAI_API_VERSION'] = '2023-05-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "   # deployment_name = 'dev-text-embedding-ada-002',\n",
    "    model = 'dev-text-embedding-ada-002' # 곧 deploy_name으로 변경될 예정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1536, 1536, 1536, 1536, 1536]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_result = embedding_model.embed_documents(\n",
    "    [\n",
    "        '안녕하세요',\n",
    "        '제 이름은 홍길동입니다.',\n",
    "        '이름은 무엇인가?',\n",
    "        '랭체인은 유용합니다.',\n",
    "        'Hello World!'\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "list(map(len, embedding_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n",
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1536, 1536)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_query_q = embedding_model.embed_query('이 대화에서 언급된 이름은 무엇입니까?')\n",
    "embedding_query_a = embedding_model.embed_query('이 대화에서 언급된 이름은 홍길동입니다.')\n",
    "\n",
    "len(embedding_query_q), len(embedding_query_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cos_sim = lambda a, b: np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n",
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8347529885108067"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(embedding_model.embed_query('너 뭐햐냐'),embedding_model.embed_query('에헤이 이거 조졌네'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9017447932333962\n",
      "0.8504563306662587\n",
      "0.7762442819388619\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(embedding_query_q, embedding_query_a))\n",
    "print(cos_sim(embedding_query_q, embedding_result[1]))\n",
    "print(cos_sim(embedding_query_q, embedding_result[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugginface 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
       "  (2): Normalize()\n",
       "), model_name='BAAI/bge-small-en', cache_folder=None, model_kwargs={'device': 'mps'}, encode_kwargs={'normalize_embeddings': True}, query_instruction='Represent this question for searching relevant passages: ')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = 'BAAI/bge-small-en'\n",
    "model_kwargs = {'device': 'mps'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # 일반화된 임베딩을 사용할 것인지 여부\n",
    "\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = model_kwargs,\n",
    "    encode_kwargs = encode_kwargs\n",
    ")\n",
    "\n",
    "hf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[384, 384, 384, 384, 384, 384]\n"
     ]
    }
   ],
   "source": [
    "embedding_result = hf.embed_documents(\n",
    "    [\n",
    "        'today is monday',\n",
    "        'weather is nice today',\n",
    "        'what is the problem?',\n",
    "        'langchain is useful',\n",
    "        'Hello World!',\n",
    "        'my name is morris'\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(list(map(len, embedding_result)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 384\n",
      "0.8522539761253092\n"
     ]
    }
   ],
   "source": [
    "BGE_query_q = hf.embed_query('Hello? who is this?')\n",
    "BGE_query_a = hf.embed_query('hi this is harrison')\n",
    "\n",
    "print(len(BGE_query_q), len(BGE_query_a))\n",
    "print(cos_sim(BGE_query_q, BGE_query_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 384\n",
      "0.7677968113692798\n"
     ]
    }
   ],
   "source": [
    "BGE_query_q = hf.embed_query('Hello? who is this?')\n",
    "BGE_query_a = hf.embed_query('I am an idiot')\n",
    "\n",
    "print(len(BGE_query_q), len(BGE_query_a))\n",
    "print(cos_sim(BGE_query_q, BGE_query_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "modules.json: 100%|██████████| 229/229 [00:00<00:00, 1.02MB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 123/123 [00:00<00:00, 947kB/s]\n",
      "README.md: 100%|██████████| 4.46k/4.46k [00:00<00:00, 6.39MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 198kB/s]\n",
      "config.json: 100%|██████████| 620/620 [00:00<00:00, 2.12MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 443M/443M [00:47<00:00, 9.29MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 538/538 [00:00<00:00, 1.53MB/s]\n",
      "vocab.txt: 100%|██████████| 248k/248k [00:00<00:00, 474kB/s]\n",
      "tokenizer.json: 100%|██████████| 495k/495k [00:00<00:00, 928kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 380kB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 594kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'jhgan/ko-sbert-nli'\n",
    "model_kwargs = {'device': 'mps'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # 일반화된 임베딩을 사용할 것인지 여부\n",
    "\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = model_kwargs,\n",
    "    encode_kwargs = encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[768, 768, 768, 768, 768]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\n",
    "    '안녕하세요',\n",
    "    '제 이름은 홍길동입니다.',\n",
    "    '이름은 무엇인가요?',\n",
    "    '랭체인은 유용합니다.',\n",
    "    '홍길동 아버지의 이름은 홍판서입니다.',\n",
    "]\n",
    "\n",
    "embedding_result = hf.embed_documents(sentence)\n",
    "\n",
    "print(list(map(len, embedding_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8996751103853661\n"
     ]
    }
   ],
   "source": [
    "q = hf.embed_query('홍길동은 아버지를 아버지라 부르지 못했습니다. 홍길동 아버지의 이름은 무엇인가요?')\n",
    "a = hf.embed_query('홍길동은 어머니를 어머니라 부르지 못했습니다. 홍길동 어머니의 이름은 무엇인가요?')\n",
    "\n",
    "print(cos_sim(q, a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skt_fly_ai_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
